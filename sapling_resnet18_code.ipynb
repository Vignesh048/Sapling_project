{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 1 :** Removed Excess dog,cat,fish,butterfly images\n",
        "\n"
      ],
      "metadata": {
        "id": "kzQGOBVZxDt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1m1B9I5I-ucYvxOJy2X65c4rL_4ueoNKs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnKg7E8_L7nE",
        "outputId": "15d6417b-04ee-44d0-8cdf-edf15a34875e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_iDBCu211qGuRmnCrkPo8uTxJG5unLmP\n",
            "To: /content/sapling_final.zip\n",
            "100% 264M/264M [00:04<00:00, 53.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 2 :** With Excess dog,cat,fish,butterfly images\n"
      ],
      "metadata": {
        "id": "1VLhgAJcxb45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1xNALY1-J6HoQYKYPapY_r82Bsr0Mim0W"
      ],
      "metadata": {
        "id": "cgt_PKT0xl73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1m1B9I5I-ucYvxOJy2X65c4rL_4ueoNKs"
      ],
      "metadata": {
        "id": "N3TweRD8xg1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rD07KBl5xYeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/sapling_final/"
      ],
      "metadata": {
        "id": "luGEL9eaDFb6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/sapling_final.zip"
      ],
      "metadata": {
        "id": "ekQ1VVh2FIdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To remove one corrupted image\n",
        "!rm /content/sapling_final/train/Saplings/PE_693.jpg"
      ],
      "metadata": {
        "id": "RBr4xQ3jUZ7S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to dataset folders\n",
        "train_folder = \"/content/sapling_final/train\"\n",
        "test_folder = \"/content/sapling_final/test\"\n",
        "val_folder = \"/content/sapling_final/val\"\n",
        "\n",
        "# Function to count images in a folder\n",
        "def count_images(folder):\n",
        "    count_per_category = {}\n",
        "    for category in os.listdir(folder):\n",
        "        category_folder = os.path.join(folder, category)\n",
        "        if os.path.isdir(category_folder):\n",
        "            images = os.listdir(category_folder)\n",
        "            count_per_category[category] = len(images)\n",
        "    return count_per_category\n",
        "\n",
        "# Count images in train folder\n",
        "train_counts = count_images(train_folder)\n",
        "\n",
        "# Count images in test folder\n",
        "test_counts = count_images(test_folder)\n",
        "\n",
        "# Count images in val folder\n",
        "val_counts = count_images(val_folder)\n",
        "\n",
        "# Print the counts per category in train folder\n",
        "print(\"Train:\")\n",
        "for category, count in train_counts.items():\n",
        "    print(f\"{category}: {count}\")\n",
        "\n",
        "# Print the counts per category in test folder\n",
        "print(\"\\nTest:\")\n",
        "for category, count in test_counts.items():\n",
        "    print(f\"{category}: {count}\")\n",
        "\n",
        "# Print the counts per category in val folder\n",
        "print(\"\\nValidation:\")\n",
        "for category, count in val_counts.items():\n",
        "    print(f\"{category}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KGpXI-yMTzR",
        "outputId": "04fb78b8-9a49-4ad3-8eaa-3b0660964674"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:\n",
            "Saplings: 663\n",
            "Non-Saplings: 2020\n",
            "\n",
            "Test:\n",
            "Saplings: 132\n",
            "Non-Saplings: 404\n",
            "\n",
            "Validation:\n",
            "Saplings: 88\n",
            "Non-Saplings: 269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pretrained ResNet-18 model\n",
        "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "num_features = model.fc.in_features\n",
        "\n",
        "# Freeze pretrained weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Add two more fully connected layers\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 1)\n",
        ")  # Binary classification, output layer with 1 neuron\n",
        "\n",
        "# Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=3.0, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = nn.BCEWithLogitsLoss()(inputs, targets.view(-1, 1).float())\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "        return focal_loss.mean()\n"
      ],
      "metadata": {
        "id": "iGhJzxOAUC4-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = FocalLoss()\n",
        "# Define the data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to a consistent size\n",
        "    transforms.ToTensor(),           # Convert images to tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image data\n",
        "])\n",
        "\n",
        "train_folder = \"/content/sapling_final/train\"\n",
        "test_folder = \"/content/sapling_final/test\"\n",
        "val_folder = \"/content/sapling_final/val\"\n",
        "# Load your dataset and perform necessary transformations\n",
        "train_dataset = ImageFolder(train_folder, transform=transform)\n",
        "test_dataset = ImageFolder(test_folder, transform=transform)\n",
        "val_dataset = ImageFolder(val_folder, transform=transform)\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 16\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.5f}\")\n",
        "\n",
        "        # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            predicted = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.reshape((labels.size(0))) == labels).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwvVKFRWyI3u",
        "outputId": "8736fedb-1f7a-4165-e7b8-7d04ea9b0c6f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.00956\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 2/30, Loss: 0.00636\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 3/30, Loss: 0.00698\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 4/30, Loss: 0.00665\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 5/30, Loss: 0.00711\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 6/30, Loss: 0.00803\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 7/30, Loss: 0.00823\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 8/30, Loss: 0.00662\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 9/30, Loss: 0.00736\n",
            "Validation Accuracy: 0.9664\n",
            "Epoch 10/30, Loss: 0.00681\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 11/30, Loss: 0.00658\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 12/30, Loss: 0.00602\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 13/30, Loss: 0.00772\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 14/30, Loss: 0.00666\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 15/30, Loss: 0.00521\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 16/30, Loss: 0.00563\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 17/30, Loss: 0.00502\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 18/30, Loss: 0.00659\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 19/30, Loss: 0.00720\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 20/30, Loss: 0.00758\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 21/30, Loss: 0.00465\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 22/30, Loss: 0.00666\n",
            "Validation Accuracy: 0.9664\n",
            "Epoch 23/30, Loss: 0.00646\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 24/30, Loss: 0.00805\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 25/30, Loss: 0.00867\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 26/30, Loss: 0.00809\n",
            "Validation Accuracy: 0.9664\n",
            "Epoch 27/30, Loss: 0.00815\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 28/30, Loss: 0.00653\n",
            "Validation Accuracy: 0.9720\n",
            "Epoch 29/30, Loss: 0.00598\n",
            "Validation Accuracy: 0.9692\n",
            "Epoch 30/30, Loss: 0.00633\n",
            "Validation Accuracy: 0.9720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluation\n",
        "# test_folder = \"/content/pg\"\n",
        "\n",
        "# test_dataset = ImageFolder(test_folder, transform=transform)\n",
        "\n",
        "\n",
        "\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.reshape((labels.size(0))) == labels).sum().item()\n",
        "        # print(predicted.reshape((labels.size(0))).shape,labels.shape)\n",
        "\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bQI86jPyVe6",
        "outputId": "6bb5db16-b68b-40b7-9c3c-e86b7321b0de"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9757\n",
            "Confusion Matrix:\n",
            "[[394  10]\n",
            " [  3 129]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "cm = f1_score(all_labels, all_predictions)\n",
        "print(\"f1_score:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "elndHKUQoVGZ",
        "outputId": "fda00419-8761-4f26-b29e-051ef739e558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_score:\n",
            "0.9520295202952029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"sapling.h5\"\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "GyQeqglV9yaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'sapling.pth'  # Path to save the model\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "uwsA099Zzu0h"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total"
      ],
      "metadata": {
        "id": "_UlOx7dK59Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "WXVsIf735-aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "client = storage.Client(\"skilful-voltage-300413\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3trMV55XpgJ",
        "outputId": "2c671e0e-c78b-4364-cb8a-17f789b66283"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket = client.get_bucket(\"sapling_bucket\")"
      ],
      "metadata": {
        "id": "hA9K0WJvX0H-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = bucket.blob(\"sapling.pth\")\n",
        "blob.upload_from_filename('/content/sapling.pth')"
      ],
      "metadata": {
        "id": "g-2pGUljX30Q"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pretrained ResNet-18 model\n",
        "model1 = models.resnet18(pretrained=True)\n",
        "num_features = model1.fc.in_features\n",
        "\n",
        "# Freeze pretrained weights\n",
        "for param in model1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Add two more fully connected layers\n",
        "model1.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 1)\n",
        ")  # Binary classification, output layer with 1 neuron\n",
        "\n",
        "import os\n",
        "model_path = 'sapling.pth'  # Path to the saved model\n",
        "model1.load_state_dict(torch.load(model_path))\n",
        "model1.eval()\n",
        "\n",
        "\n",
        "  # Preprocess the input image\n",
        "image_folder = '/content/pg1/pg2/'\n",
        "  # Replace with the path to your input image\n",
        "for images in os.listdir(image_folder):\n",
        "    image_path = os.path.join(image_folder, images)\n",
        "    print(images)\n",
        "    image = Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize images to a consistent size\n",
        "        transforms.ToTensor(),           # Convert images to tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image data\n",
        "    ])\n",
        "\n",
        "    input_tensor = transform(image)\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        output = model1(input_batch)\n",
        "\n",
        "    # Convert sigmoid probabilities to binary predictions\n",
        "    predicted = torch.sigmoid(output)\n",
        "    predicted_labels = (predicted >= 0.5).int()\n",
        "\n",
        "    # Get the predicted class label (0 or 1)\n",
        "    predicted_class = predicted_labels.item()\n",
        "\n",
        "    if predicted_class == 1:\n",
        "\n",
        "      predicted_class_name = \"Sapling\"\n",
        "\n",
        "    else:\n",
        "\n",
        "      predicted_class_name = \"Not a Sapling\"\n",
        "\n",
        "\n",
        "    print(f\"Predicted class: {predicted_class_name}\")\n",
        "    print(predicted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdvjViMEYJuo",
        "outputId": "9ce8c1c7-ce7d-4e31-afb3-83cce3ef12a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new1.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.2271]])\n",
            "new10.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.4563]])\n",
            "new13.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.3262]])\n",
            "new8.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.4772]])\n",
            "new4.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.3808]])\n",
            "new7.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.2570]])\n",
            "new5.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.1233]])\n",
            "new2.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.1255]])\n",
            "new6.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.4532]])\n",
            "new3.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.1417]])\n",
            "new12.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.4217]])\n",
            "new11.jpeg\n",
            "Predicted class: Sapling\n",
            "tensor([[0.7852]])\n",
            "new9.jpeg\n",
            "Predicted class: Not a Sapling\n",
            "tensor([[0.3237]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/images/images_trainval2017.zip\n"
      ],
      "metadata": {
        "id": "XF80eEQdZZFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0c70ed-a678-4497-a700-b320492b45fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-21 12:41:07--  http://images.cocodataset.org/images/images_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.128.153, 52.217.129.169, 3.5.9.158, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.128.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-06-21 12:41:07 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Define the URL of the image to download\n",
        "image_url = \"http://images.cocodataset.org/zips/train2017.zip\"\n",
        "\n",
        "# Send a GET request to download the image\n",
        "response = requests.get(image_url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Get the content of the response (image data)\n",
        "    image_data = response.content\n",
        "\n",
        "    # Save the image to a file\n",
        "    with open(\"train2017.zip\", \"wb\") as file:\n",
        "        file.write(image_data)\n",
        "\n",
        "    print(\"Image downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download image.\")\n"
      ],
      "metadata": {
        "id": "OL1AiZvw-Hle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBv20WWA_FCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}